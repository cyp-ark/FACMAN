import eventlet
eventlet.monkey_patch()

import os
from flask import Flask, render_template, request, jsonify, send_file
from flask_socketio import SocketIO
from influxdb_client import InfluxDBClient
from dotenv import load_dotenv
from flask_cors import CORS
from docx import Document
from io import BytesIO
from docx.shared import Inches
import base64
import openai
from collections import defaultdict
from datetime import datetime, timezone, timedelta
import json
import traceback

# ===============================
# ÌôòÍ≤Ω Î≥ÄÏàò Î∞è ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï
# ===============================
load_dotenv()

app = Flask(__name__)
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*", async_mode="eventlet")

INFLUX_URL = os.getenv("INFLUX_URL")
INFLUX_TOKEN = os.getenv("INFLUX_TOKEN")
INFLUX_ORG = os.getenv("INFLUX_ORG")
influx_client = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)
openai.api_key = os.getenv("OPENAI_API_KEY")

# ===============================
# ÌïúÍ∏Ä Í∏∞Í∞Ñ Î¨∏ÏûêÏó¥ Î≥ÄÌôò Ìï®Ïàò
# ===============================
def normalize_range(range_str):
    kor_to_influx = {
        "1ÏãúÍ∞Ñ": "1h", "3ÏãúÍ∞Ñ": "3h", "6ÏãúÍ∞Ñ": "6h", "9ÏãúÍ∞Ñ": "9h",
        "1Ïùº": "1d", "7Ïùº": "7d", "31Ïùº": "31d"
    }
    return kor_to_influx.get(range_str, range_str)

# ===============================
# Ïã§ÏãúÍ∞Ñ ÏÉÅÌÉú Ï°∞Ìöå Î∞è Ï†ÑÏÜ°
# ===============================
def get_recent_status(bucket):
    query = f'''
    from(bucket: "{bucket}")
      |> range(start: -30s)
      |> filter(fn: (r) => r._measurement == "status_log" and r._field == "event_type")
      |> sort(columns: ["_time"], desc: true)
      |> limit(n: 3)
    '''
    result = influx_client.query_api().query(org=INFLUX_ORG, query=query)
    events = [record.get_value() for table in result for record in table.records]
    return events if events else None

def emit_status():
    prev_events = {"P1-A": None, "P1-B": None, "P2-A": None, "P2-B": None}
    while True:
        for key in prev_events.keys():
            events = get_recent_status(f"{key}_status")
            if events:
                latest = events[0]
                if latest != prev_events[key]:
                    socketio.emit('status_update', {key: {'event_type': latest}})
                    prev_events[key] = latest
        socketio.sleep(1)

@socketio.on('connect')
def handle_connect():
    for key in ["P1", "P2"]:
        events = get_recent_status(f"{key}_status")
        if events:
            latest = events[0]
            socketio.emit('status_update', {f"{key}-A": {'event_type': latest}})

# ===============================
# ÎùºÏö∞ÌåÖ
# ===============================
@app.route("/")
def index():
    return render_template("index.html")

@app.route("/report")
def report_page():
    return render_template("report.html")

# ===============================
# Î≥¥Í≥†ÏÑú ÏÉùÏÑ± API (Îã§Ï§ë Í≥µÏ†ï ÎåÄÏùë)
# ===============================
@app.route("/generate_report", methods=["POST"])
def generate_report():
    try:
        data = request.get_json()
        processes = data.get("processes", [])
        range_str = normalize_range(data.get("range"))

        all_reports = []
        for process in processes:
            if "/" in range_str:
                start, end = range_str.split("/")
                range_clause = f'|> range(start: time(v: "{start}"), stop: time(v: "{end}"))'
                start_time = datetime.fromisoformat(start.replace("+09:00", ""))
                end_time = datetime.fromisoformat(end.replace("+09:00", ""))
            else:
                range_clause = f'|> range(start: -{range_str})'
                start_time, end_time = None, None

            query = f'''
            from(bucket: "{process}_status")
              {range_clause}
              |> filter(fn: (r) => r._measurement == "status_log")
              |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
              |> keep(columns: ["_time", "available", "event_type"])
            '''
            tables = influx_client.query_api().query(query)

            total, available_sum, failure_count = 0, 0, 0
            time_labels, available_values, failure_values = [], [], []
            failure_hourly = defaultdict(int)

            KST = timezone(timedelta(hours=9))

            for table in tables:
                for record in table.records:
                    event_type = record.get_value()
                    timestamp = record.get_time().astimezone(KST).replace(tzinfo=None)

                    # 1Ô∏è‚É£ Í∏∞Í∞Ñ ÌïÑÌÑ∞
                    if start and end:
                        if not (start <= timestamp <= end):
                            continue

                    # 2Ô∏è‚É£ Ïö¥ÏòÅ ÏãúÍ∞Ñ ÌïÑÌÑ∞ (09Ïãú~18Ïãú)
                    if not (9 <= timestamp.hour < 18):
                        continue

                    # 3Ô∏è‚É£ Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨
                    if event_type in ["failure", "maintenance"]:
                        current_event = event_type
                        current_start = timestamp

                    elif event_type == "processing" and current_event and current_start:
                        diff = (timestamp - current_start).total_seconds() / 60  # Î∂Ñ Îã®ÏúÑ
                        hour_label = current_start.strftime("%HÏãú")

                        if current_event == "failure":
                            failure_total += diff
                            failure_by_hour[hour_label] += diff
                        elif current_event == "maintenance":
                            maintenance_total += diff
                            maintenance_by_hour[hour_label] += diff

                        current_event = None
                        current_start = None

                    available = record.values.get("available", 0)
                    event_type = record.values.get("event_type", "")
                    timestamp = record_time.strftime("%H:%M")
                    hour_label = record_time.strftime("%HÏãúÎåÄ")

                    total += 1
                    available_sum += available
                    if event_type == "failure":
                        failure_count += 1
                        failure_hourly[hour_label] += 1

                    time_labels.append(timestamp)
                    available_values.append(round(available, 2))
                    failure_values.append(1 if event_type == "failure" else 0)

            avg_avail = round((available_sum / total) * 100, 1) if total else 0
            failure_table_labels = list(failure_hourly.keys())
            failure_table_counts = list(failure_hourly.values())

            prompt = f"""
            Í≥µÏ†ïÎ™Ö: {process}
            Í∏∞Í∞Ñ: ÏµúÍ∑º {range_str}
            Í∞ÄÎèôÎ•† ÌèâÍ∑†: {avg_avail}%
            Í≥†Ïû• ÌöüÏàò: {failure_count}Ìöå

            ÏúÑ Îç∞Ïù¥ÌÑ∞Î•º Î∞îÌÉïÏúºÎ°ú Ï†úÏ°∞ Í≥µÏ†ï Î≥¥Í≥†ÏÑúÎ•º ÏûëÏÑ±Ìï¥Ï§ò. Îã§Ïùå Ìï≠Î™©ÏùÑ Ìè¨Ìï®Ìï¥Ï§ò:
            1. Í≥µÏ†ï ÏöîÏïΩ
            2. Ï£ºÏöî Ïù¥Ïäà
            3. ÎåÄÏùë Ï°∞Ïπò
            4. Ìñ•ÌõÑ Ï†úÏñ∏
            """
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ÎÑàÎäî Ï†úÏ°∞Í≥µÏ†ï Î≥¥Í≥†ÏÑúÎ•º ÏûëÏÑ±ÌïòÎäî AI ÎπÑÏÑúÏïº."},
                    {"role": "user", "content": prompt}
                ]
            )
            all_reports.append({
                "process": process,
                "report": response.choices[0].message.content,
                "labels": time_labels,
                "available": available_values,
                "failures": failure_values,
                "failureLabels": failure_table_labels,
                "failureCounts": failure_table_counts
            })

        return jsonify({"reports": all_reports})
    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


# ===============================
# DOCX Îã§Ïö¥Î°úÎìú API (ÏãúÍ∞ÑÎåÄ Í∑∏Î£πÌïë Î∞òÏòÅ)
# ===============================
from flask import request

@app.route("/generate_docx", methods=["POST"])
def generate_docx():
    try:
        text = request.form.get("report", "")
        failure_labels = request.form.get("failureLabels", "[]")
        failure_counts = request.form.get("failureCounts", "[]")

        doc = Document()
        doc.add_heading("üìÑ Ïä§ÎßàÌä∏ Ï†úÏ°∞ Î≥¥Í≥†ÏÑú", 0)
        doc.add_paragraph(text)

        # ‚úÖ Ïù¥ÎØ∏ÏßÄ Ï∂îÍ∞Ä
        avail_imgs = request.files.getlist("availabilityImages")
        fail_imgs = request.files.getlist("failureImages")

        for i in range(len(avail_imgs)):
            doc.add_paragraph(f"‚úÖ [{i+1}] Í∞ÄÎèôÎ•† Î≥ÄÌôî")
            doc.add_picture(BytesIO(avail_imgs[i].read()), width=Inches(4))

        for i in range(len(fail_imgs)):
            doc.add_paragraph(f"üìä [{i+1}] Í≥†Ïû• Î∞úÏÉù Î∂ÑÌè¨")
            doc.add_picture(BytesIO(fail_imgs[i].read()), width=Inches(4))

        # ‚úÖ Í≥†Ïû• ÌÖåÏù¥Î∏î
        import json
        labels = json.loads(failure_labels)
        counts = json.loads(failure_counts)

        hour_map = {}
        for label, count in zip(labels, counts):
            hour = label[:2] + "ÏãúÎåÄ"
            hour_map[hour] = hour_map.get(hour, 0) + count

        doc.add_paragraph("üìä Í≥†Ïû• Î∞úÏÉù Î∂ÑÌè¨ ÌÖåÏù¥Î∏î (ÏãúÍ∞ÑÎåÄ Í∏∞Ï§Ä)")
        table = doc.add_table(rows=1, cols=2)
        hdr_cells = table.rows[0].cells
        hdr_cells[0].text = 'ÏãúÍ∞ÑÎåÄ'
        hdr_cells[1].text = 'Í≥†Ïû• Ïàò'
        for hour, count in hour_map.items():
            row_cells = table.add_row().cells
            row_cells[0].text = hour
            row_cells[1].text = str(count)

        output = BytesIO()
        doc.save(output)
        output.seek(0)
        return send_file(output, as_attachment=True, download_name="Ï†úÏ°∞_Î≥¥Í≥†ÏÑú.docx")
    except Exception as e:
        print(f"üìÑ DOCX ÏÉùÏÑ± Ïò§Î•ò: {e}")
        return jsonify({"error": "ÌååÏùº ÏÉùÏÑ± Ïã§Ìå®"}), 500


# ===============================
# Îã§Ïö¥ÌÉÄÏûÑ Í≥ÑÏÇ∞ API
# ===============================
@app.route("/get_downtime_data", methods=["POST"])
def get_downtime_data():
    from datetime import datetime, timezone, timedelta
    from collections import defaultdict

    data = request.json
    process = data.get("process")
    range_str = data.get("range")

    if not process or not range_str:
        return jsonify({"error": "Missing process or range"}), 400

    # ÏãúÍ∞Ñ Î≤îÏúÑ ÌååÏã±
    if "/" in range_str:
        start_str, end_str = range_str.split("/")
        start = datetime.fromisoformat(start_str.replace("Z", "").replace("+09:00", ""))
        end = datetime.fromisoformat(end_str.replace("Z", "").replace("+09:00", ""))
        range_clause = f'|> range(start: time(v: "{start_str}"), stop: time(v: "{end_str}"))'
    else:
        range_clause = f'|> range(start: -{range_str})'
        now = datetime.utcnow()
        if "h" in range_str:
            hours = int(range_str.replace("h", ""))
            start = now - timedelta(hours=hours)
        elif "d" in range_str:
            days = int(range_str.replace("d", ""))
            start = now - timedelta(days=days)
        end = now

    query = f'''
    from(bucket: "{process}_status")
      {range_clause}
      |> filter(fn: (r) => r._measurement == "status_log" and r._field == "event_type")
      |> sort(columns: ["_time"])
    '''

    tables = influx_client.query_api().query(org=INFLUX_ORG, query=query)

    # Îã§Ïö¥ÌÉÄÏûÑ ÎàÑÏ†Å
    failure_total = 0
    maintenance_total = 0
    failure_by_hour = defaultdict(int)
    maintenance_by_hour = defaultdict(int)

    current_event = None
    current_start = None

    KST = timezone(timedelta(hours=9))

    for table in tables:
        for record in table.records:
            event_type = record.get_value()
            timestamp = record.get_time().astimezone(KST).replace(tzinfo=None)

            if event_type in ["failure", "maintenance"]:
                current_event = event_type
                current_start = timestamp
            elif event_type == "processing" and current_event and current_start:
                diff = (timestamp - current_start).total_seconds() / 60  # Î∂Ñ Îã®ÏúÑ
                hour_label = current_start.strftime("%HÏãú")

                if current_event == "failure":
                    failure_total += diff
                    failure_by_hour[hour_label] += diff
                elif current_event == "maintenance":
                    maintenance_total += diff
                    maintenance_by_hour[hour_label] += diff

                current_event = None
                current_start = None

    return jsonify({
        "failure_total": round(failure_total, 1),
        "maintenance_total": round(maintenance_total, 1),
        "hourly_labels": sorted(set(list(failure_by_hour.keys()) + list(maintenance_by_hour.keys()))),
        "failure_by_hour": [round(failure_by_hour[h], 1) for h in sorted(failure_by_hour.keys())],
        "maintenance_by_hour": [round(maintenance_by_hour[h], 1) for h in sorted(maintenance_by_hour.keys())]
    })


# ===============================
# Í∞ÄÎèôÎ•† Í≥ÑÏÇ∞ API
# ===============================
@app.route("/calculate_availability", methods=["POST"])
def calculate_availability():
    try:
        data = request.json
        process = data.get("process")  # ÏòàÏãú: "P1-A"
        period = data.get("period")    # "ÏùºÍ∞Ñ", "Ï£ºÍ∞Ñ", "ÏõîÍ∞Ñ"

        # Í∏∞Í∞Ñ ÏÑ§Ï†ï
        now = datetime.utcnow()
        if period == "ÏùºÍ∞Ñ":
            start = now - timedelta(hours=24)
        elif period == "Ï£ºÍ∞Ñ":
            start = now - timedelta(days=7)
        elif period == "ÏõîÍ∞Ñ":
            start = now - timedelta(days=31)
        else:
            return jsonify({"error": "Invalid period"}), 400

        start_str = start.isoformat() + "Z"
        end_str = now.isoformat() + "Z"

        # ÏøºÎ¶¨Î¨∏
        query = f'''
        from(bucket: "{process}_status")
          |> range(start: {start_str}, stop: {end_str})
          |> filter(fn: (r) => r._measurement == "status_log" and r._field == "available")
        '''

        tables = influx_client.query_api().query(org=INFLUX_ORG, query=query)

        # Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Î∞è Í≥ÑÏÇ∞
        total, available_sum = 0, 0
        for table in tables:
            for record in table.records:
                timestamp = record.get_time().astimezone(KST).replace(tzinfo=None)
                hour = timestamp.hour

                # ‚úÖ Ïö¥ÏòÅ ÏãúÍ∞Ñ ÌïÑÌÑ∞: 09Ïãú ~ 18ÏãúÎßå Ìè¨Ìï®
                if not (9 <= hour < 18):
                    continue

                value = record.get_value()
                if value is not None:
                    total += 1
                    available_sum += value

        availability = round((available_sum / total) * 100, 1) if total else 0

        return jsonify({
            "process": process,
            "period": period,
            "availability": availability
        })

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ===============================
# Í≥†Ïû•Í±¥Ïàò API Ï∂îÍ∞Ä
# ===============================
@app.route("/calculate_failure_count", methods=["POST"])
def calculate_failure_count():
    try:
        data = request.json
        process = data.get("process")  # Ïòà: "P1-A"
        period = data.get("period")    # "ÏùºÍ∞Ñ", "Ï£ºÍ∞Ñ", "ÏõîÍ∞Ñ"

        # Í∏∞Í∞Ñ ÏÑ§Ï†ï
        now = datetime.utcnow()
        if period == "ÏùºÍ∞Ñ":
            start = now - timedelta(hours=24)
        elif period == "Ï£ºÍ∞Ñ":
            start = now - timedelta(days=7)
        elif period == "ÏõîÍ∞Ñ":
            start = now - timedelta(days=31)
        else:
            return jsonify({"error": "Invalid period"}), 400

        start_str = start.isoformat() + "Z"
        end_str = now.isoformat() + "Z"

        # ÏøºÎ¶¨Î¨∏
        query = f'''
        from(bucket: "{process}_status")
        |> range(start: {start_str}, stop: {end_str})
        |> filter(fn: (r) => r._measurement == "status_log" and r._field == "event_type" and r._value == "failure")
        '''

        tables = influx_client.query_api().query(org=INFLUX_ORG, query=query)

        KST = timezone(timedelta(hours=9))
        failure_count = 0

        for table in tables:
            for record in table.records:
                timestamp = record.get_time().astimezone(KST).replace(tzinfo=None)
                hour = timestamp.hour

                # ‚úÖ Ïö¥ÏòÅ ÏãúÍ∞ÑÎßå Ìè¨Ìï®
                if not (9 <= hour < 18):
                    continue

                failure_count += 1

        return jsonify({
            "process": process,
            "period": period,
            "failure_count": failure_count
        })

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ===============================
# ÏÑúÎ≤Ñ Ïã§Ìñâ
# ===============================
if __name__ == "__main__":
    socketio.start_background_task(target=emit_status)
    socketio.run(app, host='0.0.0.0', port=5000, debug=False, use_reloader=False)

